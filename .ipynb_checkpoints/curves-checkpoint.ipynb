{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9687b887",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"></ul></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "c9dbe29c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import multiprocessing\n",
    "from tensordict.nn import TensorDictModule\n",
    "from tensordict.nn.distributions import NormalParamExtractor\n",
    "from torch import nn\n",
    "\n",
    "from torchrl.collectors import SyncDataCollector\n",
    "from torchrl.data.replay_buffers import ReplayBuffer\n",
    "from torchrl.data.replay_buffers.samplers import SamplerWithoutReplacement\n",
    "from torchrl.data.replay_buffers.storages import LazyTensorStorage\n",
    "from torchrl.envs import (\n",
    "    Compose,\n",
    "    DoubleToFloat,\n",
    "    ObservationNorm,\n",
    "    StepCounter,\n",
    "    TransformedEnv,\n",
    ")\n",
    "from torchrl.envs.libs.gym import GymEnv\n",
    "from torchrl.envs.utils import check_env_specs, ExplorationType, set_exploration_type\n",
    "from torchrl.modules import ProbabilisticActor, TanhNormal, ValueOperator\n",
    "from torchrl.objectives import ClipPPOLoss\n",
    "from torchrl.objectives.value import GAE\n",
    "from tqdm import tqdm\n",
    "\n",
    "import gym\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "f607b216",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "5b319115",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_states = env.observation_space.shape[0]\n",
    "num_actions = env.action_space.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "da33a4f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PolicyNetwork(nn.Module):\n",
    "    def __init__(self, num_inputs, num_actions, hidden_size=128):\n",
    "        super(PolicyNetwork, self).__init__()\n",
    "        self.fc1 = nn.Linear(num_inputs, hidden_size)\n",
    "        self.fc2 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.fc3 = nn.Linear(hidden_size, num_actions)\n",
    "\n",
    "    def forward(self, state):\n",
    "        x = torch.relu(self.fc1(state))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        x = torch.tanh(self.fc3(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "aeed96f9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "policy_network = PolicyNetwork(num_states, num_actions).to(device)\n",
    "optimizer = optim.Adam(policy_network.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "540ef34f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def choose_action(state):\n",
    "    state = torch.FloatTensor([state]).to(device)\n",
    "    action = policy_network(state)\n",
    "    return action.cpu().detach().numpy().flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "325e0c8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''def play_episode(env, policy_network, max_steps=1000):\n",
    "    state = env.reset()\n",
    "    episode_reward = 0\n",
    "\n",
    "    states = []\n",
    "    actions = []\n",
    "    rewards = []\n",
    "\n",
    "    for step in range(max_steps):\n",
    "        action = choose_action(state)\n",
    "        next_state, reward, done, _ = env.step(action)\n",
    "\n",
    "        states.append(state)\n",
    "        actions.append(action)\n",
    "        rewards.append(reward)\n",
    "\n",
    "        episode_reward += reward\n",
    "\n",
    "        if done:\n",
    "            break\n",
    "        state = next_state\n",
    "\n",
    "    return states, actions, rewards, episode_reward'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "5a618355",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "expected sequence of length 24 at dim 2 (got 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[46], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m episode_rewards \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m episode \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_episodes):\n\u001b[0;32m----> 5\u001b[0m     states, actions, rewards, episode_reward \u001b[38;5;241m=\u001b[39m \u001b[43mplay_episode\u001b[49m\u001b[43m(\u001b[49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpolicy_network\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      6\u001b[0m     episode_rewards\u001b[38;5;241m.\u001b[39mappend(episode_reward)\n\u001b[1;32m      8\u001b[0m     \u001b[38;5;66;03m# Entraînement du modèle\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[45], line 10\u001b[0m, in \u001b[0;36mplay_episode\u001b[0;34m(env, policy_network, max_steps)\u001b[0m\n\u001b[1;32m      7\u001b[0m rewards \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m step \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(max_steps):\n\u001b[0;32m---> 10\u001b[0m     action \u001b[38;5;241m=\u001b[39m \u001b[43mchoose_action\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     11\u001b[0m     next_state, reward, done, _ \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mstep(action)\n\u001b[1;32m     13\u001b[0m     states\u001b[38;5;241m.\u001b[39mappend(state)\n",
      "Cell \u001b[0;32mIn[44], line 2\u001b[0m, in \u001b[0;36mchoose_action\u001b[0;34m(state)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mchoose_action\u001b[39m(state):\n\u001b[0;32m----> 2\u001b[0m     state \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mFloatTensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mto(device)  \u001b[38;5;66;03m# Convertir l'état en un tableau 2D\u001b[39;00m\n\u001b[1;32m      3\u001b[0m     action \u001b[38;5;241m=\u001b[39m policy_network(state)\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m action\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mnumpy()\u001b[38;5;241m.\u001b[39mflatten()\n",
      "\u001b[0;31mValueError\u001b[0m: expected sequence of length 24 at dim 2 (got 0)"
     ]
    }
   ],
   "source": [
    "'''num_episodes = 500\n",
    "episode_rewards = []\n",
    "\n",
    "for episode in range(num_episodes):\n",
    "    states, actions, rewards, episode_reward = play_episode(env, policy_network)\n",
    "    episode_rewards.append(episode_reward)\n",
    "\n",
    "    # Entraînement du modèle\n",
    "    policy_network.train()\n",
    "    advantages = torch.tensor(calculate_advantages(rewards)).to(device)\n",
    "    action_probs = torch.tensor([choose_action(state) for state in states], dtype=torch.float32).to(device)\n",
    "    selected_action_probs = torch.gather(action_probs, 1, torch.tensor(actions).unsqueeze(1).to(device)).squeeze()\n",
    "    loss = -torch.sum(torch.log(selected_action_probs) * advantages)\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    print(f\"Episode {episode + 1}: Reward: {episode_reward}\")'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c29e578d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tracer les courbes d'apprentissage\n",
    "plt.plot(episode_rewards)\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Rewards')\n",
    "plt.title('Learning Curve')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rlnavenv",
   "language": "python",
   "name": "rlnavenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
